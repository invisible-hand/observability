{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45694424-d90a-46d8-9b09-382fb00c7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain langgraph langchain_openai reportlab numexpr python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7626b11f-758c-4bdd-b5de-8ffc2f255762",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU openinference-instrumentation-langchain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860bf391-0829-42a7-ad8a-2b4b8655f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, List\n",
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# For PDF Generation\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "\n",
    "# For Calculator\n",
    "import numexpr\n",
    "\n",
    "# Load API keys (optional, assumes OPENAI_API_KEY is set in environment)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "print(\"OPENAI_API_KEY loaded:\", os.getenv(\"OPENAI_API_KEY\") is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad0f9b1-3b4f-400a-aa44-b8c24e6c041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90f2b20-f4c8-4c80-af38-96ae71ed95a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: langgraph\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'api_key': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tracer_provider = register(\n",
    "  project_name=\"langgraph\", # Default is 'default'\n",
    "  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a19d3f-708c-4140-aee0-5896ca2e4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates a simple mathematical expression string (e.g., \"2 * (3 + 5)\").\n",
    "    Args:\n",
    "        expression: The mathematical expression string to evaluate.\n",
    "    Returns:\n",
    "        The numerical result as a string.\n",
    "    \"\"\"\n",
    "    # Directly evaluate, assuming input is a valid string expression for numexpr\n",
    "    # No error handling - will raise exception on invalid input\n",
    "    result = numexpr.evaluate(expression).item()\n",
    "    return str(result) # Return the result directly as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e11ecc6-c83c-456e-8771-8ac97f5eac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def save_to_pdf(content: str, filename: str = \"output.pdf\") -> str:\n",
    "    \"\"\"\n",
    "    Saves the given text content to a PDF file in the '{PDF_OUTPUT_DIR}' directory.\n",
    "    Use this tool *only* when the user explicitly asks to save the previous conversation or answer as a PDF.\n",
    "    Args:\n",
    "        content: The text content to save.\n",
    "        filename: The desired name for the PDF file (e.g., 'summary.pdf'). Defaults to 'output.pdf'.\n",
    "    Returns:\n",
    "        A confirmation message or an error message.\n",
    "    \"\"\"\n",
    "    PDF_OUTPUT_DIR = \".\"\n",
    "    print(f\"üõ†Ô∏è Calling Save to PDF Tool with filename: {filename}\")\n",
    "    try:\n",
    "        # Ensure filename has .pdf extension\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            filename += \".pdf\"\n",
    "        \n",
    "        # Sanitize filename (basic example, consider more robust sanitization)\n",
    "        filename = os.path.basename(filename) # Prevent directory traversal\n",
    "        filepath = os.path.join(PDF_OUTPUT_DIR, filename)\n",
    "\n",
    "        doc = SimpleDocTemplate(filepath, pagesize=letter)\n",
    "        styles = getSampleStyleSheet()\n",
    "        # Replace newlines with <br/> tags for paragraph breaks in PDF\n",
    "        story = [Paragraph(content.replace('\\n', '<br/>'), styles['Normal'])]\n",
    "        doc.build(story)\n",
    "        return f\"Content successfully saved to {filepath}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error saving content to PDF '{filename}': {e}\"\n",
    "\n",
    "# List of tools available to the agent\n",
    "tools = [calculator, save_to_pdf]\n",
    "\n",
    "# Helper: Create a mapping from tool name to tool function\n",
    "tool_map = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d75791-1fab-46c9-9590-edf100f13063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state for our graph\n",
    "class AgentState(TypedDict):\n",
    "    # Messages queue: Stores the conversation history (Human, AI, Tool messages)\n",
    "    # operator.add makes it so new messages are appended to the list\n",
    "    messages: Annotated[List[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc46b247-2b1f-4ed6-914b-f89904409fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized and tools bound.\n"
     ]
    }
   ],
   "source": [
    "# --- LLM and Tool Binding ---\n",
    "# Use a model that supports tool calling\n",
    "# Make sure OPENAI_API_KEY is set in your environment\n",
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) \n",
    "    # Bind tools to LLM\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    print(\"LLM initialized and tools bound.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LLM. Check API key and model availability: {e}\")\n",
    "    llm_with_tools = None # Set to None to prevent errors later if init fails\n",
    "\n",
    "# --- Node Functions ---\n",
    "\n",
    "# 1. Agent Node: Calls the LLM to decide the next action or generate a response\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"Invokes the LLM to get the next step or response.\"\"\"\n",
    "    print(\"---AGENT NODE---\")\n",
    "    if llm_with_tools is None:\n",
    "         raise ValueError(\"LLM not initialized. Cannot proceed.\")\n",
    "    # The LLM's response is based on the current message history\n",
    "    response = llm_with_tools.invoke(state['messages'])\n",
    "    print(f\"LLM Response: {response.content}\")\n",
    "    if response.tool_calls:\n",
    "        print(f\"LLM requested tools: {response.tool_calls}\")\n",
    "    # The response (AIMessage with potential tool_calls) is added to the state\n",
    "    # by LangGraph thanks to the `operator.add` in AgentState definition\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 2. Tool Node: Executes the tools called by the LLM\n",
    "def tool_node(state: AgentState):\n",
    "    \"\"\"Executes tools requested by the LLM and returns their outputs.\"\"\"\n",
    "    print(\"---TOOL NODE---\")\n",
    "    last_message = state['messages'][-1] # Get the latest message (should be AIMessage with tool calls)\n",
    "\n",
    "    # Check if the last message is an AIMessage and has tool calls\n",
    "    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
    "        print(\"Warning: Tool node called without tool calls in the last message.\")\n",
    "        return {} # No tools to execute\n",
    "\n",
    "    tool_messages = []\n",
    "    print(f\"Executing tools: {[tc['name'] for tc in last_message.tool_calls]}\")\n",
    "\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call['name']\n",
    "        if tool_name in tool_map:\n",
    "            selected_tool = tool_map[tool_name]\n",
    "            tool_input = tool_call['args']\n",
    "\n",
    "            print(f\"  - Calling tool '{tool_name}' with args: {tool_input}\")\n",
    "            try:\n",
    "                # Invoke the tool. .invoke() handles both regular functions and Runnables\n",
    "                tool_output = selected_tool.invoke(tool_input)\n",
    "                print(f\"  - Tool '{tool_name}' output: {tool_output}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - Error executing tool {tool_name}: {e}\")\n",
    "                tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "\n",
    "            # Create a ToolMessage with the output and add it to our list\n",
    "            tool_messages.append(\n",
    "                ToolMessage(content=str(tool_output), tool_call_id=tool_call['id'])\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  - Warning: Tool '{tool_name}' not found.\")\n",
    "            # Still add a ToolMessage indicating the error\n",
    "            tool_messages.append(\n",
    "                ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=tool_call['id'])\n",
    "            )\n",
    "\n",
    "    # Return the list of ToolMessages to be appended to the state\n",
    "    return {\"messages\": tool_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0609f953-1854-4dd3-b207-9b1fd2021ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: AgentState) -> str:\n",
    "    \"\"\"Decides the next step based on the last message.\"\"\"\n",
    "    print(\"---ROUTER---\")\n",
    "    last_message = state['messages'][-1]\n",
    "\n",
    "    # Check 1 : Is the last message an AIMessage with tool calls?\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        print(\"Routing: Agent requested tools -> tool_node\")\n",
    "        return \"tool_node\" # Route to the tool node to execute tools\n",
    "\n",
    "    # Check 2 : Is the last message a ToolMessage?\n",
    "    if isinstance(last_message, ToolMessage):\n",
    "         print(\"Routing: Tools executed -> agent_node\")\n",
    "         return \"agent_node\" # Route back to the agent node to process tool results\n",
    "\n",
    "    # Check 3 : Is the last message an AIMessage *without* tool calls?\n",
    "    if isinstance(last_message, AIMessage) and not last_message.tool_calls:\n",
    "        print(\"Routing: Agent provided final answer for this turn -> end\")\n",
    "        return \"end\" # End the current flow, wait for next user input\n",
    "\n",
    "    # Default/Fallback (should ideally not be hit often in this structure)\n",
    "    print(\"Routing: Fallback -> end\")\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f0c5d2-4d41-4489-bc8c-0203af8e7713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a new StateGraph with our defined AgentState\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add the nodes to the graph\n",
    "workflow.add_node(\"agent_node\", agent_node)\n",
    "workflow.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Set the entry point: The first step after user input is always the agent node\n",
    "workflow.set_entry_point(\"agent_node\")\n",
    "\n",
    "# Add conditional edges based on the router's decision\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent_node\",  # Start node\n",
    "    router,        # Function that decides the next node\n",
    "    {\n",
    "        \"tool_node\": \"tool_node\", # If router returns \"tool_node\", go to tool_node\n",
    "        \"end\": END,               # If router returns \"end\", finish the graph execution\n",
    "        # Note: We don't need agent_node here as router logic doesn't loop back immediately\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add a regular edge from the tool_node back to the agent_node\n",
    "# After tools are executed, the agent needs to process their results\n",
    "workflow.add_edge(\"tool_node\", \"agent_node\")\n",
    "\n",
    "# Compile the graph into a runnable application\n",
    "try:\n",
    "    app = workflow.compile()\n",
    "    print(\"Graph compiled successfully!\")\n",
    "    # Optional: Visualize the graph (requires graphviz)\n",
    "    # from IPython.display import Image, display\n",
    "    # try:\n",
    "    #     display(Image(app.get_graph().draw_mermaid_png()))\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Graph visualization failed (is graphviz installed?): {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error compiling graph: {e}\")\n",
    "    app = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe943eb-4606-4db5-9d32-04a92e735b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Agent Interaction (type 'quit' or 'exit' to stop) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m conversation_history = [] \u001b[38;5;66;03m# Start with an empty history for each run\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Modified prompt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUser: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_input.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     11\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExiting agent interaction.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/new_env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/new_env/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "if app is None:\n",
    "    print(\"Graph compilation failed. Cannot run the agent.\")\n",
    "else:\n",
    "    print(\"\\n--- Starting Agent Interaction (type 'quit' or 'exit' to stop) ---\")\n",
    "    conversation_history = [] # Start with an empty history for each run\n",
    "\n",
    "    while True:\n",
    "        # Modified prompt\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            print(\"Exiting agent interaction.\")\n",
    "            break\n",
    "\n",
    "        if not user_input.strip():\n",
    "            continue\n",
    "\n",
    "        # Add user message to history\n",
    "        new_human_message = HumanMessage(content=user_input)\n",
    "        # Append to existing history for this session\n",
    "        conversation_history.append(new_human_message)\n",
    "\n",
    "        # Prepare the input state for the graph invocation\n",
    "        # We pass the *entire* current conversation history\n",
    "        initial_state = {\n",
    "            \"messages\": conversation_history\n",
    "        }\n",
    "\n",
    "        print(\"\\n--- Running Graph ---\")\n",
    "        # Use stream to observe the flow step-by-step\n",
    "        # Set a recursion limit for safety\n",
    "        events = app.stream(initial_state, {\"recursion_limit\": 15})\n",
    "\n",
    "        final_state_messages = []\n",
    "        # Track messages added *during this specific graph run*\n",
    "        messages_from_this_run = []\n",
    "\n",
    "        for event in events:\n",
    "            step_name = list(event.keys())[0]\n",
    "\n",
    "            if step_name != \"__end__\":\n",
    "                # Accumulate messages from intermediate steps of *this* run\n",
    "                if \"messages\" in event[step_name]:\n",
    "                      messages_from_this_run.extend(event[step_name][\"messages\"])\n",
    "\n",
    "            # Check for the final state when the graph naturally ends for this turn\n",
    "            if \"__end__\" in event:\n",
    "                final_state_data = event[\"__end__\"]\n",
    "                # The final state contains the *complete* message list up to this point\n",
    "                final_state_messages = final_state_data.get(\"messages\", [])\n",
    "                print(\"--- Graph Execution Finished ---\")\n",
    "                break # Stop processing events for this turn\n",
    "\n",
    "        # Update conversation history with the final list from the graph run\n",
    "        # If the graph finished properly, final_state_messages has the full history\n",
    "        # If it somehow stopped early, we might need to manually combine (less ideal)\n",
    "        conversation_history = final_state_messages if final_state_messages else conversation_history + messages_from_this_run\n",
    "\n",
    "\n",
    "        # Print the *last* AI response from this turn\n",
    "        last_ai_response = next((msg for msg in reversed(conversation_history) if isinstance(msg, AIMessage)), None)\n",
    "\n",
    "        if last_ai_response:\n",
    "            print(\"\\nAssistant:\")\n",
    "            if last_ai_response.content:\n",
    "                print(last_ai_response.content)\n",
    "            if last_ai_response.tool_calls:\n",
    "                 print(f\"(Debug: Requested tools: {last_ai_response.tool_calls})\")\n",
    "        else:\n",
    "            # This might happen if the graph ended without producing an AI message\n",
    "            print(\"\\nAssistant: (No new response generated for this turn)\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"--- Agent Interaction Ended ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdc195-5f33-4d4e-b2b0-d7fb81ba95c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
